{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation and Results Analysis\n",
    "\n",
    "This notebook evaluates trained models, analyzes results, and visualizes predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Data processing\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# Deep learning\n",
    "import torch\n",
    "from matplotlib.patches import Rectangle\n",
    "from PIL import Image\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# YOLO\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration and Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "PROJECT_ROOT = Path('/home/user/qontinui-finetune')\n",
    "DATASET_PATH = PROJECT_ROOT / 'data' / 'dataset'  # Update with actual dataset path\n",
    "RUNS_DIR = PROJECT_ROOT / 'runs'\n",
    "CHECKPOINTS_DIR = RUNS_DIR / 'checkpoints'\n",
    "\n",
    "# Configuration\n",
    "evaluation_config = {\n",
    "    'conf_threshold': 0.25,  # Confidence threshold for detection\n",
    "    'iou_threshold': 0.45,   # IoU threshold for NMS\n",
    "    'imgsz': 640,\n",
    "    'batch_size': 16,\n",
    "    'device': 0 if torch.cuda.is_available() else 'cpu',\n",
    "}\n",
    "\n",
    "# Dataset YAML path\n",
    "dataset_yaml_path = PROJECT_ROOT / 'dataset.yaml'\n",
    "\n",
    "print(\"Evaluation Configuration:\")\n",
    "for key, value in evaluation_config.items():\n",
    "    print(f\"  {key:20s}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and load best model\n",
    "checkpoint_files = list(CHECKPOINTS_DIR.glob('best_*.pt'))\n",
    "\n",
    "if checkpoint_files:\n",
    "    # Use most recent checkpoint\n",
    "    model_path = sorted(checkpoint_files, key=os.path.getctime, reverse=True)[0]\n",
    "    print(f\"Loading model: {model_path}\")\n",
    "    model = YOLO(str(model_path))\n",
    "    print(\"Model loaded successfully!\")\n",
    "\n",
    "    # Display model info\n",
    "    print(f\"\\nModel device: {next(model.model.parameters()).device}\")\n",
    "    print(f\"Model parameters: {sum(p.numel() for p in model.model.parameters()):,}\")\n",
    "else:\n",
    "    print(\"No checkpoint found. Please train a model first.\")\n",
    "    model_path = None\n",
    "    model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation on test set\n",
    "if model is not None and dataset_yaml_path.exists():\n",
    "    print(\"Running evaluation on test set...\")\n",
    "\n",
    "    eval_results = model.val(\n",
    "        data=str(dataset_yaml_path),\n",
    "        imgsz=evaluation_config['imgsz'],\n",
    "        batch=evaluation_config['batch_size'],\n",
    "        conf=evaluation_config['conf_threshold'],\n",
    "        iou=evaluation_config['iou_threshold'],\n",
    "        device=evaluation_config['device'],\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    print(\"\\nEvaluation completed!\")\n",
    "    print(\"\\nKey Metrics:\")\n",
    "    print(f\"  mAP@0.5: {eval_results.box.map50:.4f}\")\n",
    "    print(f\"  mAP@0.5:0.95: {eval_results.box.map:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Per-Class Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_per_class_performance(results):\n",
    "    \"\"\"Extract and analyze per-class metrics from evaluation results.\"\"\"\n",
    "\n",
    "    # Get stats if available\n",
    "    if hasattr(results, 'box') and hasattr(results.box, 'ap_class_index'):\n",
    "        class_indices = results.box.ap_class_index\n",
    "        ap_values = results.box.ap if hasattr(results.box, 'ap') else None\n",
    "\n",
    "        if ap_values is not None:\n",
    "            return class_indices, ap_values\n",
    "\n",
    "    return None, None\n",
    "\n",
    "# Analyze per-class performance\n",
    "if model is not None and 'eval_results' in locals():\n",
    "    class_indices, ap_values = analyze_per_class_performance(eval_results)\n",
    "\n",
    "    if class_indices is not None and ap_values is not None:\n",
    "        print(\"Per-Class Performance:\")\n",
    "        for idx, ap in zip(class_indices, ap_values, strict=False):\n",
    "            print(f\"  Class {idx}: AP = {ap:.4f}\")\n",
    "    else:\n",
    "        print(\"Per-class metrics not available in current format\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Detection Predictions on Test Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_predictions(model, test_images_dir, conf_threshold=0.25):\n",
    "    \"\"\"Run predictions on a directory of images.\"\"\"\n",
    "    results = []\n",
    "\n",
    "    image_files = list(test_images_dir.glob('*.jpg')) + list(test_images_dir.glob('*.png'))\n",
    "\n",
    "    for img_path in image_files:\n",
    "        # Run inference\n",
    "        pred = model.predict(\n",
    "            source=str(img_path),\n",
    "            conf=conf_threshold,\n",
    "            imgsz=evaluation_config['imgsz'],\n",
    "            device=evaluation_config['device'],\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "        results.append({\n",
    "            'image_path': img_path,\n",
    "            'predictions': pred[0] if pred else None\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "# Run predictions on test set\n",
    "if model is not None:\n",
    "    test_images_dir = DATASET_PATH / 'images' / 'test'\n",
    "\n",
    "    if test_images_dir.exists():\n",
    "        print(f\"Running predictions on test set: {test_images_dir}\")\n",
    "        print(f\"Number of test images: {len(list(test_images_dir.glob('*.jpg')) + list(test_images_dir.glob('*.png')))}\")\n",
    "\n",
    "        predictions = run_predictions(model, test_images_dir, evaluation_config['conf_threshold'])\n",
    "        print(f\"Predictions completed for {len(predictions)} images\")\n",
    "    else:\n",
    "        print(f\"Test images directory not found: {test_images_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction(pred_result, class_names=None):\n",
    "    \"\"\"Plot prediction results with bounding boxes.\"\"\"\n",
    "    if pred_result['predictions'] is None:\n",
    "        return None\n",
    "\n",
    "    img_path = pred_result['image_path']\n",
    "    pred = pred_result['predictions']\n",
    "\n",
    "    # Read image\n",
    "    img = Image.open(img_path)\n",
    "    img_array = np.array(img)\n",
    "    img_h, img_w = img_array.shape[:2]\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "    ax.imshow(img_array)\n",
    "\n",
    "    # Draw bounding boxes\n",
    "    if hasattr(pred, 'boxes') and pred.boxes is not None:\n",
    "        boxes = pred.boxes\n",
    "\n",
    "        for _i, box in enumerate(boxes):\n",
    "            # Get box coordinates (xyxy format)\n",
    "            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
    "            conf = box.conf[0].cpu().numpy()\n",
    "            cls = int(box.cls[0].cpu().numpy())\n",
    "\n",
    "            # Draw rectangle\n",
    "            rect = Rectangle((x1, y1), x2-x1, y2-y1,\n",
    "                            linewidth=2, edgecolor='lime', facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "            # Draw label\n",
    "            class_name = class_names.get(cls, f'Class {cls}') if class_names else f'Class {cls}'\n",
    "            label_text = f'{class_name}: {conf:.2f}'\n",
    "            ax.text(x1, y1 - 8, label_text, fontsize=10,\n",
    "                   color='white', bbox={'facecolor': 'lime', 'alpha': 0.8})\n",
    "\n",
    "    ax.set_title(f\"Predictions: {img_path.name}\", fontsize=12, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "\n",
    "    return fig\n",
    "\n",
    "# Visualize sample predictions\n",
    "if 'predictions' in locals():\n",
    "    # Sample up to 5 predictions\n",
    "    for pred in predictions[:5]:\n",
    "        plot_prediction(pred)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Confusion Matrix Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_confusion_matrix(predictions, labels_dir, num_classes=2):\n",
    "    \"\"\"Create confusion matrix from predictions.\"\"\"\n",
    "    all_true_labels = []\n",
    "    all_pred_labels = []\n",
    "\n",
    "    for pred_result in predictions:\n",
    "        img_path = pred_result['image_path']\n",
    "        pred = pred_result['predictions']\n",
    "\n",
    "        # Get true labels from file\n",
    "        label_file = labels_dir / (img_path.stem + '.txt')\n",
    "        true_classes = []\n",
    "\n",
    "        if label_file.exists():\n",
    "            with open(label_file) as f:\n",
    "                for line in f:\n",
    "                    class_id = int(line.split()[0])\n",
    "                    true_classes.append(class_id)\n",
    "\n",
    "        # Get predicted labels\n",
    "        pred_classes = []\n",
    "        if hasattr(pred, 'boxes') and pred.boxes is not None:\n",
    "            for box in pred.boxes:\n",
    "                cls = int(box.cls[0].cpu().numpy())\n",
    "                pred_classes.append(cls)\n",
    "\n",
    "        # Match predictions to ground truth (simple matching)\n",
    "        all_true_labels.extend(true_classes)\n",
    "        all_pred_labels.extend(pred_classes[:len(true_classes)])\n",
    "\n",
    "    # Create confusion matrix\n",
    "    if all_true_labels and all_pred_labels:\n",
    "        cm = confusion_matrix(all_true_labels, all_pred_labels,\n",
    "                              labels=list(range(num_classes)))\n",
    "        return cm\n",
    "\n",
    "    return None\n",
    "\n",
    "# Create and visualize confusion matrix\n",
    "if 'predictions' in locals():\n",
    "    labels_dir = DATASET_PATH / 'labels' / 'test'\n",
    "    cm = create_confusion_matrix(predictions, labels_dir)\n",
    "\n",
    "    if cm is not None:\n",
    "        fig, ax = plt.subplots(figsize=(8, 7))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True,\n",
    "                   xticklabels=list(range(cm.shape[0])),\n",
    "                   yticklabels=list(range(cm.shape[0])),\n",
    "                   ax=ax)\n",
    "        ax.set_xlabel('Predicted Label', fontsize=12)\n",
    "        ax.set_ylabel('True Label', fontsize=12)\n",
    "        ax.set_title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Detection Confidence Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_confidence_scores(predictions):\n",
    "    \"\"\"Analyze confidence scores of predictions.\"\"\"\n",
    "    all_confidences = []\n",
    "    confidence_by_class = defaultdict(list)\n",
    "\n",
    "    for pred_result in predictions:\n",
    "        pred = pred_result['predictions']\n",
    "\n",
    "        if hasattr(pred, 'boxes') and pred.boxes is not None:\n",
    "            for box in pred.boxes:\n",
    "                conf = box.conf[0].cpu().numpy()\n",
    "                cls = int(box.cls[0].cpu().numpy())\n",
    "\n",
    "                all_confidences.append(conf)\n",
    "                confidence_by_class[cls].append(conf)\n",
    "\n",
    "    return all_confidences, confidence_by_class\n",
    "\n",
    "# Analyze confidence scores\n",
    "if 'predictions' in locals():\n",
    "    all_confidences, conf_by_class = analyze_confidence_scores(predictions)\n",
    "\n",
    "    if all_confidences:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "        # Overall confidence distribution\n",
    "        axes[0].hist(all_confidences, bins=30, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "        axes[0].axvline(np.mean(all_confidences), color='red', linestyle='--',\n",
    "                        linewidth=2, label=f'Mean: {np.mean(all_confidences):.3f}')\n",
    "        axes[0].set_xlabel('Confidence Score', fontsize=11)\n",
    "        axes[0].set_ylabel('Frequency', fontsize=11)\n",
    "        axes[0].set_title('Detection Confidence Distribution', fontsize=12, fontweight='bold')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "        # Confidence by class (box plot)\n",
    "        class_labels = sorted(conf_by_class.keys())\n",
    "        class_confidences = [conf_by_class[c] for c in class_labels]\n",
    "\n",
    "        bp = axes[1].boxplot(class_confidences, labels=[f'Class {c}' for c in class_labels],\n",
    "                             patch_artist=True)\n",
    "        for patch in bp['boxes']:\n",
    "            patch.set_facecolor('lightblue')\n",
    "        axes[1].set_ylabel('Confidence Score', fontsize=11)\n",
    "        axes[1].set_title('Confidence by Class', fontsize=12, fontweight='bold')\n",
    "        axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Print statistics\n",
    "        print(\"Overall Confidence Statistics:\")\n",
    "        print(f\"  Mean: {np.mean(all_confidences):.4f}\")\n",
    "        print(f\"  Std: {np.std(all_confidences):.4f}\")\n",
    "        print(f\"  Min: {np.min(all_confidences):.4f}\")\n",
    "        print(f\"  Max: {np.max(all_confidences):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Export Model for Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export model to different formats\n",
    "if model is not None:\n",
    "    export_dir = PROJECT_ROOT / 'exports'\n",
    "    export_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    print(\"Exporting model to different formats...\\n\")\n",
    "\n",
    "    # Supported export formats\n",
    "    export_formats = {\n",
    "        'onnx': 'ONNX (Open Neural Network Exchange)',\n",
    "        'torchscript': 'TorchScript',\n",
    "        'tflite': 'TensorFlow Lite',\n",
    "    }\n",
    "\n",
    "    # Export to ONNX\n",
    "    try:\n",
    "        print(\"Exporting to ONNX format...\")\n",
    "        onnx_path = model.export(format='onnx', imgsz=evaluation_config['imgsz'])\n",
    "        print(f\"  Success: {onnx_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Failed: {e}\")\n",
    "\n",
    "    # Export to TorchScript\n",
    "    try:\n",
    "        print(\"\\nExporting to TorchScript format...\")\n",
    "        torchscript_path = model.export(format='torchscript', imgsz=evaluation_config['imgsz'])\n",
    "        print(f\"  Success: {torchscript_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Failed: {e}\")\n",
    "\n",
    "    print(f\"\\nExports saved to: {export_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Inference Speed Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def benchmark_inference(model, test_images_dir, num_images=10):\n",
    "    \"\"\"Benchmark inference speed.\"\"\"\n",
    "    image_files = list(test_images_dir.glob('*.jpg')) + list(test_images_dir.glob('*.png'))\n",
    "    image_files = image_files[:num_images]\n",
    "\n",
    "    inference_times = []\n",
    "\n",
    "    print(f\"Benchmarking inference on {len(image_files)} images...\\n\")\n",
    "\n",
    "    for _i, img_path in enumerate(image_files):\n",
    "        start = time.time()\n",
    "\n",
    "        model.predict(\n",
    "            source=str(img_path),\n",
    "            conf=evaluation_config['conf_threshold'],\n",
    "            imgsz=evaluation_config['imgsz'],\n",
    "            device=evaluation_config['device'],\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "        elapsed = time.time() - start\n",
    "        inference_times.append(elapsed)\n",
    "\n",
    "    return inference_times\n",
    "\n",
    "# Run benchmark\n",
    "if model is not None:\n",
    "    test_images_dir = DATASET_PATH / 'images' / 'test'\n",
    "\n",
    "    if test_images_dir.exists():\n",
    "        inference_times = benchmark_inference(model, test_images_dir, num_images=10)\n",
    "\n",
    "        print(\"\\nInference Speed Statistics:\")\n",
    "        print(f\"  Mean: {np.mean(inference_times)*1000:.2f} ms\")\n",
    "        print(f\"  Std: {np.std(inference_times)*1000:.2f} ms\")\n",
    "        print(f\"  Min: {np.min(inference_times)*1000:.2f} ms\")\n",
    "        print(f\"  Max: {np.max(inference_times)*1000:.2f} ms\")\n",
    "        print(f\"  FPS: {1/np.mean(inference_times):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Comprehensive Evaluation Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive evaluation report\n",
    "if model is not None:\n",
    "    print(\"=\"*70)\n",
    "    print(\"MODEL EVALUATION REPORT\".center(70))\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    print(f\"\\nModel: {model_path.name if model_path else 'Unknown'}\")\n",
    "    print(f\"Device: {evaluation_config['device']}\")\n",
    "    print(f\"Input Size: {evaluation_config['imgsz']}x{evaluation_config['imgsz']}\")\n",
    "\n",
    "    if 'eval_results' in locals():\n",
    "        print(\"\\n[Evaluation Results]\")\n",
    "        print(f\"  mAP@0.5: {eval_results.box.map50:.4f}\")\n",
    "        print(f\"  mAP@0.5:0.95: {eval_results.box.map:.4f}\")\n",
    "\n",
    "    if 'inference_times' in locals():\n",
    "        print(\"\\n[Inference Performance]\")\n",
    "        print(f\"  Mean Inference Time: {np.mean(inference_times)*1000:.2f} ms\")\n",
    "        print(f\"  Throughput: {1/np.mean(inference_times):.2f} FPS\")\n",
    "\n",
    "    if 'all_confidences' in locals():\n",
    "        print(\"\\n[Detection Confidence]\")\n",
    "        print(f\"  Mean Confidence: {np.mean(all_confidences):.4f}\")\n",
    "        print(f\"  Std: {np.std(all_confidences):.4f}\")\n",
    "\n",
    "    print(f\"\\nExports: {PROJECT_ROOT / 'exports'}\")\n",
    "    print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mime_type": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
