{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training Experiments and Model Fine-tuning\n",
        "\n",
        "This notebook handles model training, configuration, and real-time monitoring of training metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Environment Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard library imports\n",
        "import os\n",
        "import json\n",
        "import yaml\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Data processing\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Deep learning\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "# YOLO and training utilities\n",
        "from ultralytics import YOLO\n",
        "from ultralytics.utils import yaml_load, yaml_save\n",
        "\n",
        "# Configure plotting\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\nsns.set_palette('husl')\n",
        "%matplotlib inline\n",
        "\n",
        "# Check environment\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"GPU Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'N/A'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define paths\n",
        "PROJECT_ROOT = Path('/home/user/qontinui-finetune')\n",
        "DATASET_PATH = PROJECT_ROOT / 'data' / 'dataset'  # Update with actual dataset path\n",
        "RUNS_DIR = PROJECT_ROOT / 'runs'\n",
        "CHECKPOINTS_DIR = RUNS_DIR / 'checkpoints'\n",
        "LOGS_DIR = RUNS_DIR / 'logs'\n",
        "\n",
        "# Create directories\n",
        "RUNS_DIR.mkdir(exist_ok=True)\n",
        "CHECKPOINTS_DIR.mkdir(exist_ok=True)\n",
        "LOGS_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "print(\"Project structure:\")\n",
        "print(f\"  Project Root: {PROJECT_ROOT}\")\n",
        "print(f\"  Dataset: {DATASET_PATH}\")\n",
        "print(f\"  Runs: {RUNS_DIR}\")\n",
        "print(f\"  Checkpoints: {CHECKPOINTS_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Model Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training configuration\ntraining_config = {\n",
        "    'model': 'yolov8n',  # yolov8n, yolov8s, yolov8m, yolov8l, yolov8x\n",
        "    'epochs': 100,\n",
        "    'batch_size': 16,\n",
        "    'imgsz': 640,\n",
        "    'device': 0 if torch.cuda.is_available() else 'cpu',\n",
        "    'workers': 4,\n",
        "    'optimizer': 'SGD',  # SGD, Adam, AdamW\n",
        "    'lr0': 0.01,  # Initial learning rate\n",
        "    'lrf': 0.01,  # Final learning rate ratio\n",
        "    'momentum': 0.937,\n",
        "    'weight_decay': 0.0005,\n",
        "    'warmup_epochs': 3.0,\n",
        "    'warmup_momentum': 0.8,\n",
        "    'warmup_bias_lr': 0.1,\n",
        "    'augmentation': True,\n",
        "    'hsv_h': 0.015,  # HSV-Hue augmentation\n",
        "    'hsv_s': 0.7,    # HSV-Saturation augmentation\n",
        "    'hsv_v': 0.4,    # HSV-Value augmentation\n",
        "    'degrees': 10.0, # Rotation range\n",
        "    'translate': 0.1, # Translation ratio\n",
        "    'scale': 0.5,    # Scale ratio\n",
        "    'flipud': 0.0,   # Flip up-down\n",
        "    'fliplr': 0.5,   # Flip left-right\n",
        "    'mosaic': 1.0,   # Mosaic augmentation\n",
        "    'patience': 20,  # Early stopping patience\n",
        "    'save_period': 10,  # Save checkpoint every N epochs\n",
        "}\n",
        "\nprint(\"Training Configuration:\")\nfor key, value in training_config.items():\n",
        "    print(f\"  {key:20s}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Create Dataset YAML Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create dataset.yaml for YOLO\ndataset_yaml = {\n",
        "    'path': str(DATASET_PATH),\n",
        "    'train': 'images/train',\n",
        "    'val': 'images/val',\n",
        "    'test': 'images/test',\n",
        "    'nc': 2,  # Number of classes - UPDATE based on your dataset\n",
        "    'names': {0: 'class_0', 1: 'class_1'}  # Class names - UPDATE accordingly\n",
        "}\n",
        "\n# Save dataset YAML\ndataset_yaml_path = PROJECT_ROOT / 'dataset.yaml'\nwith open(dataset_yaml_path, 'w') as f:\n",
        "    yaml.dump(dataset_yaml, f, default_flow_style=False)\n",
        "\nprint(\"Dataset YAML created:\")\nprint(yaml.dump(dataset_yaml, default_flow_style=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Initialize Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load pretrained model\nmodel_name = training_config['model']\nprint(f\"Loading {model_name} model...\")\n\nmodel = YOLO(f'{model_name}.pt')\n\n# Display model info\nprint(f\"\\nModel Summary:\")\nmodel.info()\n\n# Get model device\ndevice = next(model.model.parameters()).device\nprint(f\"\\nModel device: {device}\")\nprint(f\"Model parameters: {sum(p.numel() for p in model.model.parameters()):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Training Loop with Progress Tracking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure training parameters\ntraining_args = {\n",
        "    'data': str(dataset_yaml_path),\n",
        "    'epochs': training_config['epochs'],\n",
        "    'batch': training_config['batch_size'],\n",
        "    'imgsz': training_config['imgsz'],\n",
        "    'device': training_config['device'],\n",
        "    'workers': training_config['workers'],\n",
        "    'optimizer': training_config['optimizer'],\n",
        "    'lr0': training_config['lr0'],\n",
        "    'lrf': training_config['lrf'],\n",
        "    'momentum': training_config['momentum'],\n",
        "    'weight_decay': training_config['weight_decay'],\n",
        "    'warmup_epochs': training_config['warmup_epochs'],\n",
        "    'warmup_momentum': training_config['warmup_momentum'],\n",
        "    'warmup_bias_lr': training_config['warmup_bias_lr'],\n",
        "    'hsv_h': training_config['hsv_h'],\n",
        "    'hsv_s': training_config['hsv_s'],\n",
        "    'hsv_v': training_config['hsv_v'],\n",
        "    'degrees': training_config['degrees'],\n",
        "    'translate': training_config['translate'],\n",
        "    'scale': training_config['scale'],\n",
        "    'flipud': training_config['flipud'],\n",
        "    'fliplr': training_config['fliplr'],\n",
        "    'mosaic': training_config['mosaic'],\n",
        "    'patience': training_config['patience'],\n",
        "    'save_period': training_config['save_period'],\n",
        "    'project': str(RUNS_DIR),\n",
        "    'name': f'exp_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}',\n",
        "    'exist_ok': False,\n",
        "    'verbose': True,\n",
        "    'seed': 42,\n",
        "    'deterministic': True,\n",
        "}\n",
        "\nprint(\"Starting training...\")\nprint(f\"Experiment name: {training_args['name']}\")\n\n# Train the model\nresults = model.train(**training_args)\n\nprint(\"\\nTraining completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Load Training Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the latest run directory\nrun_dirs = sorted([d for d in RUNS_DIR.glob('exp_*')], key=os.path.getmtime, reverse=True)\nif run_dirs:\n",
        "    latest_run = run_dirs[0]\n",
        "    print(f\"Latest run: {latest_run}\")\n",
        "    \n",
        "    # Load results CSV\n",
        "    results_csv = latest_run / 'results.csv'\n",
        "    if results_csv.exists():\n",
        "        results_df = pd.read_csv(results_csv)\n",
        "        print(f\"\\nResults shape: {results_df.shape}\")\n",
        "        print(f\"\\nLast 5 epochs:\")\n",
        "        print(results_df.tail())\n",
        "    else:\n",
        "        print(f\"Results CSV not found at {results_csv}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Live Loss and Metric Plotting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_training_metrics(results_df):\n",
        "    \"\"\"Plot training metrics from results dataframe.\"\"\"\n",
        "    \n",
        "    # Extract columns\n",
        "    epochs = results_df['epoch'].values if 'epoch' in results_df.columns else range(len(results_df))\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
        "    \n",
        "    # Training loss\n",
        "    if 'train/loss' in results_df.columns:\n",
        "        axes[0, 0].plot(epochs, results_df['train/loss'], 'b-o', linewidth=2, label='Train Loss')\n",
        "        axes[0, 0].set_xlabel('Epoch', fontsize=11)\n",
        "        axes[0, 0].set_ylabel('Loss', fontsize=11)\n",
        "        axes[0, 0].set_title('Training Loss', fontsize=12, fontweight='bold')\n",
        "        axes[0, 0].grid(True, alpha=0.3)\n",
        "        axes[0, 0].legend()\n",
        "    \n",
        "    # Validation loss\n",
        "    if 'val/loss' in results_df.columns:\n",
        "        axes[0, 1].plot(epochs, results_df['val/loss'], 'r-o', linewidth=2, label='Val Loss')\n",
        "        axes[0, 1].set_xlabel('Epoch', fontsize=11)\n",
        "        axes[0, 1].set_ylabel('Loss', fontsize=11)\n",
        "        axes[0, 1].set_title('Validation Loss', fontsize=12, fontweight='bold')\n",
        "        axes[0, 1].grid(True, alpha=0.3)\n",
        "        axes[0, 1].legend()\n",
        "    \n",
        "    # mAP@0.5\n",
        "    if 'metrics/mAP50' in results_df.columns:\n",
        "        axes[0, 2].plot(epochs, results_df['metrics/mAP50'], 'g-o', linewidth=2, label='mAP@0.5')\n",
        "        axes[0, 2].set_xlabel('Epoch', fontsize=11)\n",
        "        axes[0, 2].set_ylabel('mAP', fontsize=11)\n",
        "        axes[0, 2].set_title('mAP@0.5', fontsize=12, fontweight='bold')\n",
        "        axes[0, 2].grid(True, alpha=0.3)\n",
        "        axes[0, 2].legend()\n",
        "    \n",
        "    # Precision\n",
        "    if 'metrics/precision' in results_df.columns:\n",
        "        axes[1, 0].plot(epochs, results_df['metrics/precision'], 'purple', marker='o', linewidth=2, label='Precision')\n",
        "        axes[1, 0].set_xlabel('Epoch', fontsize=11)\n",
        "        axes[1, 0].set_ylabel('Precision', fontsize=11)\n",
        "        axes[1, 0].set_title('Precision', fontsize=12, fontweight='bold')\n",
        "        axes[1, 0].set_ylim([0, 1])\n",
        "        axes[1, 0].grid(True, alpha=0.3)\n",
        "        axes[1, 0].legend()\n",
        "    \n",
        "    # Recall\n",
        "    if 'metrics/recall' in results_df.columns:\n",
        "        axes[1, 1].plot(epochs, results_df['metrics/recall'], 'orange', marker='o', linewidth=2, label='Recall')\n",
        "        axes[1, 1].set_xlabel('Epoch', fontsize=11)\n",
        "        axes[1, 1].set_ylabel('Recall', fontsize=11)\n",
        "        axes[1, 1].set_title('Recall', fontsize=12, fontweight='bold')\n",
        "        axes[1, 1].set_ylim([0, 1])\n",
        "        axes[1, 1].grid(True, alpha=0.3)\n",
        "        axes[1, 1].legend()\n",
        "    \n",
        "    # mAP@0.5:0.95\n",
        "    if 'metrics/mAP50-95' in results_df.columns:\n",
        "        axes[1, 2].plot(epochs, results_df['metrics/mAP50-95'], 'brown', marker='o', linewidth=2, label='mAP@0.5:0.95')\n",
        "        axes[1, 2].set_xlabel('Epoch', fontsize=11)\n",
        "        axes[1, 2].set_ylabel('mAP', fontsize=11)\n",
        "        axes[1, 2].set_title('mAP@0.5:0.95', fontsize=12, fontweight='bold')\n",
        "        axes[1, 2].grid(True, alpha=0.3)\n",
        "        axes[1, 2].legend()\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n# Plot if results exist\nif results_csv.exists():\n",
        "    results_df = pd.read_csv(results_csv)\n",
        "    plot_training_metrics(results_df)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Checkpoint Management"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def list_checkpoints(run_dir):\n",
        "    \"\"\"List all checkpoint files in a run directory.\"\"\"\n",
        "    weights_dir = run_dir / 'weights'\n",
        "    if weights_dir.exists():\n",
        "        checkpoints = list(weights_dir.glob('*.pt'))\n",
        "        return sorted(checkpoints, key=os.path.getctime, reverse=True)\n",
        "    return []\n",
        "\nif run_dirs:\n",
        "    latest_run = run_dirs[0]\n",
        "    checkpoints = list_checkpoints(latest_run)\n",
        "    \n",
        "    print(f\"Available checkpoints in {latest_run.name}:\")\n",
        "    for checkpoint in checkpoints:\n",
        "        file_size = os.path.getsize(checkpoint) / (1024 * 1024)  # Convert to MB\n",
        "        print(f\"  - {checkpoint.name:30s} ({file_size:6.2f} MB)\")\n",
        "    \n",
        "    # Copy best model to checkpoints directory\n",
        "    best_model = latest_run / 'weights' / 'best.pt'\n",
        "    if best_model.exists():\n",
        "        import shutil\n",
        "        dest = CHECKPOINTS_DIR / f'best_{latest_run.name}.pt'\n",
        "        shutil.copy(best_model, dest)\n",
        "        print(f\"\\nBest model copied to {dest}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Model Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Validate the model on validation set\nif run_dirs:\n",
        "    latest_run = run_dirs[0]\n",
        "    best_model_path = latest_run / 'weights' / 'best.pt'\n",
        "    \n",
        "    if best_model_path.exists():\n",
        "        print(f\"Validating model: {best_model_path}\")\n",
        "        \n",
        "        # Load best model\n",
        "        val_model = YOLO(str(best_model_path))\n",
        "        \n",
        "        # Run validation\n",
        "        val_results = val_model.val(\n",
        "            data=str(dataset_yaml_path),\n",
        "            imgsz=training_config['imgsz'],\n",
        "            batch=training_config['batch_size'],\n",
        "            device=training_config['device'],\n",
        "            verbose=True,\n",
        "        )\n",
        "        \n",
        "        print(\"\\nValidation completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Training Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print training summary\nif run_dirs and results_csv.exists():\n",
        "    results_df = pd.read_csv(results_csv)\n",
        "    \n",
        "    print(\"=\"*60)\n",
        "    print(\"TRAINING SUMMARY\".center(60))\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    print(f\"\\nTotal Epochs: {len(results_df)}\")\n",
        "    \n",
        "    # Find best metrics\n",
        "    if 'metrics/mAP50' in results_df.columns:\n",
        "        best_map_idx = results_df['metrics/mAP50'].idxmax()\n",
        "        best_map = results_df.loc[best_map_idx, 'metrics/mAP50']\n",
        "        best_map_epoch = results_df.loc[best_map_idx, 'epoch']\n",
        "        print(f\"\\nBest mAP@0.5: {best_map:.4f} (Epoch {int(best_map_epoch)})\")\n",
        "    \n",
        "    if 'metrics/precision' in results_df.columns:\n",
        "        best_precision = results_df['metrics/precision'].max()\n",
        "        print(f\"Best Precision: {best_precision:.4f}\")\n",
        "    \n",
        "    if 'metrics/recall' in results_df.columns:\n",
        "        best_recall = results_df['metrics/recall'].max()\n",
        "        print(f\"Best Recall: {best_recall:.4f}\")\n",
        "    \n",
        "    if 'train/loss' in results_df.columns:\n",
        "        final_train_loss = results_df['train/loss'].iloc[-1]\n",
        "        print(f\"\\nFinal Training Loss: {final_train_loss:.4f}\")\n",
        "    \n",
        "    if 'val/loss' in results_df.columns:\n",
        "        final_val_loss = results_df['val/loss'].iloc[-1]\n",
        "        print(f\"Final Validation Loss: {final_val_loss:.4f}\")\n",
        "    \n",
        "    print(f\"\\nRun Directory: {latest_run}\")\n",
        "    print(\"=\"*60)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
